{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3283d23b",
   "metadata": {},
   "source": [
    "# **Stage-1:- Creating a Knowledge Base**\n",
    "\n",
    "- **Problem Statement**\n",
    "  - Contract / defense data is stored in Excel but itâ€™s hard to search intelligently\n",
    "  - Traditional keyword search fails when the query wording is different (synonyms / rephrasing)\n",
    "  - Finding the most relevant past contract descriptions takes too much manual effort\n",
    "  - Even if you find a match, getting the full context (supplier, program, amount, etc.) from the correct row is difficult\n",
    "  - A scalable system is needed to support quick retrieval + future AI extraction workflows\n",
    "\n",
    "- **Proposed Solution**\n",
    "  - Build a Vector Knowledge Base (KB) from the Excel dataset\n",
    "  - Convert contract descriptions into semantic embeddings using a transformer model\n",
    "  - Store embeddings in a FAISS vector index for fast similarity-based retrieval\n",
    "  - Store all original Excel columns as metadata to return complete structured information\n",
    "  - Enable searching based on meaning, not only exact words\n",
    "\n",
    "- **Outcome**\n",
    "  * Your Excel knowledge base will become **searchable by meaning (semantic search)** instead of only keywords\n",
    "  * When you give a **new contract description/query**, the system will return the **most similar past contracts** instantly\n",
    "  * You will be able to retrieve the **best-matching row** even if the words are different (synonyms, rephrasing, short forms)\n",
    "  * Along with the match, you will also get the **complete row details** (Supplier, Program, Amount, Dates, etc.) because metadata is stored\n",
    "  * Your extraction pipeline will become **more accurate**, since the LLM can be grounded with relevant historical examples\n",
    "  * It will reduce **manual lookup time**, improve consistency, and make the process scalable as data grows\n",
    "  * You will have a reusable **system KB (FAISS + metadata files)** that can be loaded anytime without rebuilding every time\n",
    "  * This becomes a strong base for building an **agentic workflow** like: Retrieve â†’ Validate â†’ Extract â†’ Store â†’ Report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf543cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading Excel Knowledge Base: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\data\\sample_data.xlsx\n",
      " Loaded rows=2068 cols=29\n",
      "KB rows kept after cleaning: 600\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.25s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.47s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.35s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.36s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.22s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.28s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.31s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.35s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.28s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (600, 384)\n",
      "\n",
      " System KB Created Successfully!\n",
      "Index saved: system_kb_store\\system_kb.faiss\n",
      "Meta saved : system_kb_store\\system_kb_meta.pkl\n",
      "\n",
      " Loading FAISS index: system_kb_store\\system_kb.faiss\n",
      "Loading metadata: system_kb_store\\system_kb_meta.pkl\n",
      "Loaded KB rows: 600\n",
      "Loading embedder: sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "============================================================\n",
      "TOP MATCHES\n",
      "============================================================\n",
      "\n",
      "ðŸ”¹ Rank: 1\n",
      "Score: 0.6228203773498535\n",
      "Supplier: Dell Inc\n",
      "Market: Unknown\n",
      "System: Department of Defense Enterprise Software Initiative (DOD ESI)\n",
      "Row ID: 379\n",
      "\n",
      "ðŸ”¹ Rank: 2\n",
      "Score: 0.6228203773498535\n",
      "Supplier: Dell Inc\n",
      "Market: Unknown\n",
      "System: Department of Defense Enterprise Software Initiative (DOD ESI)\n",
      "Row ID: 1\n",
      "\n",
      "ðŸ”¹ Rank: 3\n",
      "Score: 0.6024371385574341\n",
      "Supplier: \n",
      "Market: Unknown\n",
      "System: Department of Defense Enterprise Software Initiative (DOD ESI)\n",
      "Row ID: 159\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DEFAULT_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def safe_to_str(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "\n",
    "def build_system_kb_store_all_columns(\n",
    "    excel_path: str,\n",
    "    save_dir: str = \"system_kb_store\",\n",
    "    model_name: str = DEFAULT_MODEL_NAME,\n",
    "    batch_size: int = 64,\n",
    "    embed_column: str = \"Description of Contract\",\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n Loading Excel Knowledge Base: {excel_path}\")\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(f\" Loaded rows={len(df)} cols={len(df.columns)}\")\n",
    "\n",
    "    if embed_column not in df.columns:\n",
    "        raise ValueError(f\"Embed column '{embed_column}' not found in Excel!\")\n",
    "\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    kb_texts = []\n",
    "    kb_meta = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        desc = clean_text(row[embed_column])\n",
    "\n",
    "        # Skip weak text\n",
    "        if not desc or len(desc) < 20:\n",
    "            continue\n",
    "\n",
    "        meta = {\"row_id\": int(idx)}\n",
    "        for col in df.columns:\n",
    "            meta[col] = safe_to_str(row[col])\n",
    "\n",
    "        meta[embed_column] = desc\n",
    "\n",
    "        kb_texts.append(desc)\n",
    "        kb_meta.append(meta)\n",
    "\n",
    "    print(f\"KB rows kept after cleaning: {len(kb_texts)}\")\n",
    "\n",
    "    if len(kb_texts) == 0:\n",
    "        print(\"ERROR: No text rows remained after cleaning.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(kb_texts), batch_size):\n",
    "        batch = kb_texts[i : i + batch_size]\n",
    "        batch_emb = embedder.encode(\n",
    "            batch, show_progress_bar=True, normalize_embeddings=True\n",
    "        )\n",
    "        embeddings.append(batch_emb)\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "    # Cosine similarity via Inner Product (since normalized)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    index_path = os.path.join(save_dir, \"system_kb.faiss\")\n",
    "    meta_path = os.path.join(save_dir, \"system_kb_meta.pkl\")\n",
    "\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        pickle.dump(kb_meta, f)\n",
    "\n",
    "    print(\"\\n System KB Created Successfully!\")\n",
    "    print(f\"Index saved: {index_path}\")\n",
    "    print(f\"Meta saved : {meta_path}\")\n",
    "\n",
    "    return index_path, meta_path\n",
    "\n",
    "# Part 2: Retriever\n",
    "\n",
    "class SystemKBRetriever:\n",
    "    def __init__(self, kb_dir=\"system_kb_store\", model_name=DEFAULT_MODEL_NAME):\n",
    "        index_path = os.path.join(kb_dir, \"system_kb.faiss\")\n",
    "        meta_path = os.path.join(kb_dir, \"system_kb_meta.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_path) or not os.path.exists(meta_path):\n",
    "            raise FileNotFoundError(\"âŒ KB files missing. Build KB first.\")\n",
    "\n",
    "        print(f\"\\n Loading FAISS index: {index_path}\")\n",
    "        self.index = faiss.read_index(index_path)\n",
    "\n",
    "        print(f\"Loading metadata: {meta_path}\")\n",
    "        with open(meta_path, \"rb\") as f:\n",
    "            self.meta = pickle.load(f)\n",
    "\n",
    "        print(f\"Loaded KB rows: {len(self.meta)}\")\n",
    "        print(f\"Loading embedder: {model_name}\")\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "\n",
    "    def retrieve(self, query_text: str, top_k: int = 5):\n",
    "        query_text = str(query_text).strip()\n",
    "        if not query_text:\n",
    "            return []\n",
    "\n",
    "        q_emb = self.embedder.encode([query_text], normalize_embeddings=True).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        scores, idxs = self.index.search(q_emb, top_k)\n",
    "        results = []\n",
    "\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "\n",
    "            results.append({\"score\": float(score), \"meta\": self.meta[idx]})\n",
    "\n",
    "        return results\n",
    "\n",
    "# Run Full Pipeline (Build + Search)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EXCEL_PATH = r\"C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\data\\sample_data.xlsx\"\n",
    "    KB_DIR = \"system_kb_store\"\n",
    "\n",
    "    # Step 1: Build KB\n",
    "    build_system_kb_store_all_columns(\n",
    "        excel_path=EXCEL_PATH,\n",
    "        save_dir=KB_DIR,\n",
    "        model_name=DEFAULT_MODEL_NAME,\n",
    "        batch_size=64,\n",
    "        embed_column=\"Description of Contract\",\n",
    "    )\n",
    "\n",
    "    # Step 2: Load KB + Retrieve\n",
    "    r = SystemKBRetriever(kb_dir=KB_DIR)\n",
    "\n",
    "    query = \"Dell Marketing L.P., Round Rock, Texas, is awarded a single-award, firm-fixed-price blanket purchase agreement\"\n",
    "\n",
    "    hits = r.retrieve(query, top_k=3)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP MATCHES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, h in enumerate(hits, start=1):\n",
    "        print(f\"\\nðŸ”¹ Rank: {i}\")\n",
    "        print(\"Score:\", h[\"score\"])\n",
    "        print(\"Supplier:\", h[\"meta\"].get(\"Supplier Name\", \"\"))\n",
    "        print(\"Market:\", h[\"meta\"].get(\"Market Segment\", \"\"))\n",
    "        print(\"System:\", h[\"meta\"].get(\"System Name (Specific)\", \"\"))\n",
    "        print(\"Row ID:\", h[\"meta\"].get(\"row_id\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27531d34",
   "metadata": {},
   "source": [
    "## **Stage 2**\n",
    "\n",
    "In this stage I will be creating AI agest that can help in extraction of data based on the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab4a89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import difflib\n",
    "import datetime\n",
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import getpass\n",
    "\n",
    "# LangGraph / LangChain\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "# Excel formatting\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from prompts import (\n",
    "    GEOGRAPHY_PROMPT, \n",
    "    SYSTEM_CLASSIFIER_PROMPT, \n",
    "    CONTRACT_EXTRACTOR_PROMPT, \n",
    "    VALIDATOR_FIX_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbac0c",
   "metadata": {},
   "source": [
    "**Configurations and Supporting File Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b5cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM CLIENT SETUP (OpenRouter)\n",
    "\n",
    "if \"LLMFOUNDRY_TOKEN\" not in os.environ:\n",
    "    os.environ[\"LLMFOUNDRY_TOKEN\"] = getpass.getpass(\"Enter the LLM Foundry API Key: \")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=f'{os.environ.get(\"LLMFOUNDRY_TOKEN\")}:my-test-project',\n",
    "    base_url=\"https://llmfoundry.straive.com/openai/v1/\",\n",
    ")\n",
    "\n",
    "\n",
    "OPENROUTER_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Token control per stage\n",
    "MAXTOK_STAGE2 = 250\n",
    "MAXTOK_STAGE3 = 850\n",
    "MAXTOK_STAGE4 = 350\n",
    "MAXTOK_STAGE7 = 350\n",
    "\n",
    "# CONFIGURATION & FILE PATHS\n",
    "TAXONOMY_PATH = r\"C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\notebook\\taxonomy.json\"\n",
    "SUPPLIERS_PATH = r\"C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\notebook\\suppliers.json\"\n",
    "INPUT_EXCEL_PATH = r\"C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\data\\source_file.xlsx\"\n",
    "OUTPUT_CSV_PATH = \"Processed_Defense_Data.csv\"\n",
    "RAG_KB_DIR = r\"C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\system_kb_store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af0ae7",
   "metadata": {},
   "source": [
    "**Helper Functions for Stage-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c743241",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RULE BOOK + GEOGRAPHY\n",
    "\n",
    "RULE_BOOK = {\n",
    "    \"defensive_countermeasures\": {\n",
    "        \"triggers\": [\"flare\", \"chaff\", \"countermeasure\", \"decoy\", \"mju-\", \"ale-\"],\n",
    "        \"guidance\": \"Market Segment: 'C4ISR Systems', System Type (General): 'Defensive Systems', Specific: 'Defensive Aid Suite'\"\n",
    "    },\n",
    "    \"radars_and_sensors\": {\n",
    "        \"triggers\": [\"radar\", \"sonar\", \"sensor\", \"an/apy\", \"an/tpy\"],\n",
    "        \"guidance\": \"Market Segment: 'C4ISR Systems', System Type (General): 'Sensors'\"\n",
    "    },\n",
    "    \"ammunition\": {\n",
    "        \"triggers\": [\"cartridge\", \"round\", \"projectile\", \" 5.56\", \" 7.62\", \"ammo\"],\n",
    "        \"guidance\": \"Market Segment: 'Weapon Systems', System Type (General): 'Ammunition'\"\n",
    "    }\n",
    "}\n",
    "\n",
    "GEOGRAPHY_MAPPING = {\n",
    "    \"Sub-Saharan Africa\": [\n",
    "        \"Angola\", \"Benin\", \"Botswana\", \"Burkina Faso\", \"Burundi\", \"Cameroon\", \"Cape Verde\",\n",
    "        \"Central African Republic\", \"Chad\", \"Congo, Democratic Republic of\", \"Congo, Republic of\",\n",
    "        \"Djibouti\", \"Equatorial Guinea\", \"Eritrea\", \"Eswatini\", \"Ethiopia\", \"Gabon\", \"Gambia\",\n",
    "        \"Ghana\", \"Guinea\", \"Guinea-Bissau\", \"Ivory Coast\", \"Kenya\", \"Lesotho\", \"Liberia\",\n",
    "        \"Madagascar\", \"Malawi\", \"Mali\", \"Mauritius\", \"Mozambique\", \"Namibia\", \"Niger\",\n",
    "        \"Nigeria\", \"Rwanda\", \"Senegal\", \"Seychelles\", \"Sierra Leone\", \"Somalia\", \"South Africa\",\n",
    "        \"South Sudan\", \"Sudan\", \"Tanzania\", \"Togo\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "    ],\n",
    "    \"Asia-Pacific\": [\n",
    "        \"Australia\", \"Brunei\", \"Cambodia\", \"China\", \"Hong Kong\", \"Indonesia\", \"Japan\", \"Laos\",\n",
    "        \"Malaysia\", \"Mongolia\", \"Myanmar\", \"New Zealand\", \"North Korea\", \"Papua New Guinea\",\n",
    "        \"Philippines\", \"Singapore\", \"South Korea\", \"Taiwan\", \"Thailand\", \"Vietnam\"\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        \"Albania\", \"Austria\", \"Belgium\", \"Bosnia and Herzegovina\", \"Bulgaria\", \"Croatia\", \"Cyprus\",\n",
    "        \"Czech Republic\", \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Georgia\", \"Germany\", \"Greece\",\n",
    "        \"Hungary\", \"Iceland\", \"Ireland\", \"Italy\", \"Kosovo\", \"Latvia\", \"Lithuania\", \"Luxembourg\",\n",
    "        \"Malta\", \"Montenegro\", \"Netherlands\", \"North Macedonia\", \"Norway\", \"Poland\", \"Portugal\",\n",
    "        \"Romania\", \"Serbia\", \"Slovakia\", \"Slovenia\", \"Spain\", \"Sweden\", \"Switzerland\", \"Turkey\",\n",
    "        \"Ukraine\", \"United Kingdom\"\n",
    "    ],\n",
    "    \"Latin America\": [\n",
    "        \"Argentina\", \"Bahamas\", \"Barbados\", \"Belize\", \"Bolivia\", \"Brazil\", \"Chile\", \"Colombia\",\n",
    "        \"Costa Rica\", \"Cuba\", \"Curacao\", \"Dominican Republic\", \"Ecuador\", \"El Salvador\", \"Guatemala\",\n",
    "        \"Guyana\", \"Haiti\", \"Honduras\", \"Jamaica\", \"Mexico\", \"Nicaragua\", \"Panama\", \"Paraguay\",\n",
    "        \"Peru\", \"Suriname\", \"Trinidad and Tobago\", \"Uruguay\", \"Venezuela\"\n",
    "    ],\n",
    "    \"Middle East and North Africa\": [\n",
    "        \"Algeria\", \"Bahrain\", \"Egypt\", \"Iran\", \"Iraq\", \"Israel\", \"Jordan\", \"Kuwait\", \"Lebanon\",\n",
    "        \"Libya\", \"Mauritania\", \"Morocco\", \"Oman\", \"Qatar\", \"Saudi Arabia\", \"Syria\", \"Tunisia\",\n",
    "        \"United Arab Emirates\", \"Yemen\"\n",
    "    ],\n",
    "    \"North America\": [\"Canada\", \"USA\"],\n",
    "    \"Russia & CIS\": [\n",
    "        \"Armenia\", \"Azerbaijan\", \"Belarus\", \"Kazakhstan\", \"Kyrgyzstan\", \"Moldova\", \"Russia\",\n",
    "        \"Tajikistan\", \"Turkmenistan\", \"Uzbekistan\"\n",
    "    ],\n",
    "    \"South Asia\": [\n",
    "        \"Afghanistan\", \"Bangladesh\", \"India\", \"Maldives\", \"Nepal\", \"Pakistan\", \"Sri Lanka\"\n",
    "    ],\n",
    "    \"Unknown\": [\n",
    "        \"Andorra\", \"Antigua and Barbuda\", \"Bhutan\", \"Comoros\", \"Dominica\", \"Federated States of Micronesia\",\n",
    "        \"Fiji\", \"Grenada\", \"Kiribati\", \"Liechtenstein\", \"Marshall Islands\", \"Monaco\", \"Nauru\", \"Palau\",\n",
    "        \"Palestine\", \"Puerto Rico\", \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\",\n",
    "        \"Samoa\", \"San Marino\", \"Sao Tom and Principe\", \"Solomon Islands\", \"Timor-Leste\", \"Tonga\", \"Tuvalu\",\n",
    "        \"Unknown\", \"Vanuatu\", \"Vatican City\", \"Western Sahara\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "ALLOWED_OPERATORS = [\n",
    "    \"Army\",\n",
    "    \"Navy\",\n",
    "    \"Air Force\",\n",
    "    \"Defence Wide\",\n",
    "    \"Ukraine (Assistance)\",\n",
    "    \"Foreign Assistance\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "PROGRAM_TYPE_ALLOWED = [\n",
    "    \"Procurement\",\n",
    "    \"Training\",\n",
    "    \"MRO/Support\",\n",
    "    \"RDT&E\",\n",
    "    \"Upgrade\",\n",
    "    \"Other Service\"\n",
    "]\n",
    "\n",
    "DESIGNATOR_PATTERNS = [\n",
    "    r\"\\bDDG[-\\s]?\\d+\\b\", r\"\\bCVN[-\\s]?\\d+\\b\", r\"\\bSSN[-\\s]?\\d+\\b\",\n",
    "    r\"\\bLCS[-\\s]?\\d+\\b\", r\"\\bLPD[-\\s]?\\d+\\b\", r\"\\bLHA[-\\s]?\\d+\\b\", r\"\\bLHD[-\\s]?\\d+\\b\",\n",
    "    r\"\\bF-\\d+\\b\", r\"\\bB-\\d+\\b\", r\"\\bC-\\d+\\b\", r\"\\bA-\\d+\\b\",\n",
    "    r\"\\bMQ-\\d+\\b\", r\"\\bRQ-\\d+\\b\",\n",
    "    r\"\\bAN\\/[A-Z0-9\\-]+\\b\",\n",
    "    r\"\\b(AIM|AGM|SM|RIM|MIM)-\\d+\\b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "637b5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\notebook\\taxonomy.json\n",
      "Loaded: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\notebook\\suppliers.json\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1800, overlap: int = 250) -> List[str]:\n",
    "    \"\"\"\n",
    "    Deterministic chunking: stable + safe + prevents token overflow.\n",
    "    - chunk_size: number of characters per chunk\n",
    "    - overlap: overlapping characters to preserve boundaries\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        start = max(0, end - overlap)\n",
    "\n",
    "    return chunks\n",
    "#--------------------------------------------\n",
    "\n",
    "def pick_best_non_empty(values: List[str]) -> str:\n",
    "    \"\"\"Return first strong value else empty.\"\"\"\n",
    "    for v in values:\n",
    "        if v and str(v).strip() and str(v).strip().lower() not in [\"unknown\", \"n/a\", \"not applicable\", \"none\"]:\n",
    "            return str(v).strip()\n",
    "    return \"\"\n",
    "#--------------------------------------------\n",
    "\n",
    "def normalize_customer_operator(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strictly maps input text to the allowed SOP drop-down list.\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    text = str(raw_text).strip().lower()\n",
    "\n",
    "    # 1. Ukraine (Assistance) - High Priority\n",
    "    # Detects: Ukraine Security Assistance Initiative, USAI, or explicitly \"for Ukraine\"\n",
    "    if \"ukraine\" in text:\n",
    "        return \"Ukraine (Assistance)\"\n",
    "\n",
    "    # 2. Foreign Assistance\n",
    "    # Detects: FMS, Foreign Military Sales, Foreign Customers\n",
    "    if any(k in text for k in [\"foreign military sales\", \"fms\", \"foreign customer\", \"foreign government\"]):\n",
    "        return \"Foreign Assistance\"\n",
    "\n",
    "    # 3. Air Force (Includes Space Force per SOP)\n",
    "    if any(k in text for k in [\"space force\", \"ussf\", \"air force\", \"usaf\"]):\n",
    "        return \"Air Force\"\n",
    "\n",
    "    # 4. Navy (Includes Marine Corps as they are Dept of Navy, unless 'Other' is preferred)\n",
    "    # Note: SOP didn't explicitly list Marine Corps. Standard practice maps USMC to Navy. \n",
    "    # If strict separation is needed, move 'marine' to Other. \n",
    "    if any(k in text for k in [\"navy\", \"usn\", \"marine\", \"usmc\", \"naval\"]):\n",
    "        return \"Navy\"\n",
    "\n",
    "    # 5. Army\n",
    "    if any(k in text for k in [\"army\", \"usa \", \"u.s. army\"]): # Space padding to avoid matching \"army\" inside other words\n",
    "        return \"Army\"\n",
    "\n",
    "    # 6. Defence Wide\n",
    "    # Detects: DLA, MDA, DTRA, DARPA, DISA, DCMA\n",
    "    if any(k in text for k in [\"defense logistics\", \"dla\", \"missile defense\", \"mda\", \"defense wide\", \"defence wide\", \"darpa\", \"disa\"]):\n",
    "        return \"Defence Wide\"\n",
    "\n",
    "    # 7. Other\n",
    "    # Detects: Coast Guard (per SOP), DIA, Joint, Special Ops\n",
    "    if any(k in text for k in [\"coast guard\", \"uscg\", \"defense intelligence\", \"dia\", \"joint\", \"special operations\", \"socom\"]):\n",
    "        return \"Other\"\n",
    "\n",
    "    # Fallback to Other if it looks like a government agency but didn't match above\n",
    "    if len(text) > 2:\n",
    "        return \"Other\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "def load_json_file(filename, default_value):\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            print(f\"Loaded: {filename}\")\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load {filename} ({e}). Using default.\")\n",
    "        return default_value\n",
    "#--------------------------------------------\n",
    "\n",
    "def normalize_country_name(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardizes country names to ensure consistent Region mapping.\n",
    "    \"\"\"\n",
    "    if not c:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    t = str(c).strip().lower()\n",
    "    \n",
    "    # Common variations map\n",
    "    mappings = {\n",
    "        \"usa\": \"USA\", \"us\": \"USA\", \"u.s.\": \"USA\", \"united states\": \"USA\", \n",
    "        \"united states of america\": \"USA\", \"america\": \"USA\",\n",
    "        \"uk\": \"United Kingdom\", \"u.k.\": \"United Kingdom\", \"britain\": \"United Kingdom\", \n",
    "        \"great britain\": \"United Kingdom\", \"england\": \"United Kingdom\",\n",
    "        \"uae\": \"United Arab Emirates\",\n",
    "        \"sk\": \"South Korea\", \"rok\": \"South Korea\", \"republic of korea\": \"South Korea\",\n",
    "        \"prc\": \"China\", \"people's republic of china\": \"China\"\n",
    "    }\n",
    "\n",
    "    if t in mappings:\n",
    "        return mappings[t]\n",
    "        \n",
    "    # Return capitalized version if no match found\n",
    "    return str(c).strip().title()\n",
    "#--------------------------------------------\n",
    "\n",
    "def normalize_program_type(pt: str) -> str:\n",
    "    if not pt:\n",
    "        return \"Other Service\"\n",
    "\n",
    "    t = str(pt).strip().lower()\n",
    "\n",
    "    if any(k in t for k in [\"mro\", \"support\", \"maintenance\", \"repair\", \"overhaul\", \"sustainment\", \"logistics\"]):\n",
    "        return \"MRO/Support\"\n",
    "    if \"training\" in t:\n",
    "        return \"Training\"\n",
    "    if any(k in t for k in [\"rdte\", \"research\", \"development\", \"prototype\", \"test and evaluation\"]):\n",
    "        return \"RDT&E\"\n",
    "    if any(k in t for k in [\"upgrade\", \"modernization\", \"modification\"]):\n",
    "        return \"Upgrade\"\n",
    "    if any(k in t for k in [\"procure\", \"buy\", \"purchase\", \"production\", \"delivery\"]):\n",
    "        return \"Procurement\"\n",
    "\n",
    "    return \"Other Service\"\n",
    "#--------------------------------------------\n",
    "\n",
    "raw_taxonomy = load_json_file(TAXONOMY_PATH, {})\n",
    "TAXONOMY_STR = json.dumps(raw_taxonomy, separators=(\",\", \":\"))\n",
    "\n",
    "SUPPLIER_LIST = load_json_file(SUPPLIERS_PATH, [\n",
    "    \"Dell Inc\", \"Boeing\", \"Lockheed Martin\", \"Raytheon Technologies\",\n",
    "    \"Northrop Grumman\", \"L3Harris\", \"BAE Systems\", \"General Dynamics\"\n",
    "])\n",
    "\n",
    "#--------------------------------------------\n",
    "def get_best_supplier_match(extracted_name: str) -> str:\n",
    "    if not extracted_name or str(extracted_name).strip().lower() in [\"unknown\", \"n/a\", \"not applicable\"]:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    raw_name = str(extracted_name).strip()\n",
    "    raw_lower = raw_name.lower()\n",
    "    valid_suppliers = sorted([str(s) for s in SUPPLIER_LIST], key=len, reverse=True)\n",
    "\n",
    "    # 1) Exact match\n",
    "    for s in valid_suppliers:\n",
    "        if s.lower() == raw_lower:\n",
    "            return s\n",
    "\n",
    "    # 2) Known inside extracted\n",
    "    for s in valid_suppliers:\n",
    "        if len(s) > 3 and s.lower() in raw_lower:\n",
    "            return s\n",
    "\n",
    "    # 3) Extracted inside known\n",
    "    for s in valid_suppliers:\n",
    "        if raw_lower in s.lower():\n",
    "            return s\n",
    "\n",
    "    # 4) Strict fuzzy\n",
    "    matches = difflib.get_close_matches(raw_name, valid_suppliers, n=1, cutoff=0.8)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "\n",
    "    return raw_name\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "def extract_awardee_supplier_strict(paragraph: str) -> Tuple[str, str]:\n",
    "    text = str(paragraph).strip()\n",
    "\n",
    "    patterns = [\n",
    "        r\"^([A-Z][A-Za-z0-9&\\-\\.\\s]+?),\\s+.*?\\s+(?:is|was|has been)\\s+awarded\\b\",\n",
    "        r\"^([A-Z][A-Za-z0-9&\\-\\.\\s]+?),\\s+.*?\\s+received\\s+an?\\s+award\\b\",\n",
    "    ]\n",
    "\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            raw_supplier = m.group(1).strip()\n",
    "            final_supplier = get_best_supplier_match(raw_supplier)\n",
    "            return final_supplier, raw_supplier\n",
    "\n",
    "    return \"Unknown\", \"Not Found\"\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "def calculate_mro_months(start_date_str, end_date_text, program_type):\n",
    "    if program_type != \"MRO/Support\":\n",
    "        return \"Not Applicable\"\n",
    "    try:\n",
    "        if not start_date_str or not end_date_text:\n",
    "            return \"Not Applicable\"\n",
    "\n",
    "        start = pd.to_datetime(start_date_str, dayfirst=True)\n",
    "        end = parser.parse(str(end_date_text), fuzzy=True)\n",
    "\n",
    "        diff = relativedelta(end, start)\n",
    "        total_months = diff.years * 12 + diff.months\n",
    "        return str(max(0, int(total_months)))\n",
    "    except:\n",
    "        return \"Not Applicable\"\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "def get_region_for_country(country_name):\n",
    "    if not country_name or str(country_name).strip().lower() in [\"unknown\", \"n/a\", \"not applicable\"]:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    clean = str(country_name).strip().lower()\n",
    "\n",
    "    if clean in [\"us\", \"usa\", \"u.s.\", \"united states\", \"united states of america\"]:\n",
    "        return \"North America\"\n",
    "    if clean in [\"uk\", \"u.k.\", \"britain\", \"great britain\"]:\n",
    "        return \"Europe\"\n",
    "\n",
    "    for region, countries in GEOGRAPHY_MAPPING.items():\n",
    "        if any(c.lower() == clean for c in countries):\n",
    "            return region\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_designators(text: str):\n",
    "    text = str(text)\n",
    "    found = []\n",
    "    for pat in DESIGNATOR_PATTERNS:\n",
    "        found.extend(re.findall(pat, text, flags=re.IGNORECASE))\n",
    "    cleaned = [f.upper().replace(\" \", \"\").replace(\"--\", \"-\") for f in found]\n",
    "    final, seen = [], set()\n",
    "    for x in cleaned:\n",
    "        if x not in seen:\n",
    "            final.append(x)\n",
    "            seen.add(x)\n",
    "    return final\n",
    "\n",
    "\n",
    "def detect_piloting_rule_based(text: str, designators: List[str]) -> str:\n",
    "    t = str(text).lower()\n",
    "\n",
    "    if any(d.startswith((\"MQ-\", \"RQ-\")) for d in designators):\n",
    "        return \"Uncrewed\"\n",
    "    if any(k in t for k in [\"unmanned\", \"uav\", \"drone\", \"autonomous\"]):\n",
    "        return \"Uncrewed\"\n",
    "\n",
    "    if any(d.startswith((\"DDG\", \"CVN\", \"SSN\", \"LCS\", \"LPD\", \"LHA\", \"LHD\")) for d in designators):\n",
    "        return \"Crewed\"\n",
    "    if \"uss \" in t:\n",
    "        return \"Crewed\"\n",
    "\n",
    "    return \"Not Applicable\"\n",
    "\n",
    "def detect_piloting_strict(text: str, designators: List[str], system_type: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Strictly determines System Piloting based on SOP definitions.\n",
    "    \"\"\"\n",
    "    t = str(text).lower()\n",
    "    \n",
    "    # 1. OPTIONAL (Explicitly stated)\n",
    "    if any(k in t for k in [\"optionally manned\", \"optional piloting\", \"manned-unmanned teaming\", \"manned/unmanned\"]):\n",
    "        return \"Optional\"\n",
    "\n",
    "    # 2. UNCREWED (UAVs, Drones, Autonomous)\n",
    "    # Check designators first (Strong signal)\n",
    "    if any(d.startswith((\"MQ-\", \"RQ-\", \"XQ-\", \"MQ-\", \"RQ-\")) for d in designators):\n",
    "        return \"Uncrewed\"\n",
    "    # Check keywords\n",
    "    if any(k in t for k in [\"unmanned\", \"uav\", \"uas\", \"drone\", \"autonomous\", \"remotely piloted\", \"rpa\"]):\n",
    "        return \"Uncrewed\"\n",
    "\n",
    "    # 3. CREWED (Requires human pilot/driver)\n",
    "    # Check designators (Ships, Fighters, Bombers, Transports)\n",
    "    if any(d.startswith((\"DDG\", \"CVN\", \"SSN\", \"LCS\", \"LPD\", \"LHA\", \"LHD\", \"CG-\", \"FFG\")) for d in designators):\n",
    "        return \"Crewed\"\n",
    "    if any(d.startswith((\"F-\", \"B-\", \"C-\", \"A-\", \"AH-\", \"UH-\", \"CH-\", \"MH-\")) and not d.startswith(\"C-UAS\") for d in designators):\n",
    "        return \"Crewed\"\n",
    "    # Check keywords\n",
    "    if any(k in t for k in [\"manned\", \"crew\", \"pilot\", \"cockpit\", \"fighter aircraft\", \"helicopter\", \"submarine\", \"frigate\", \"destroyer\", \"carrier\"]):\n",
    "        # Exclusion: \"unmanned\" check above handles \"unmanned surface vessel\"\n",
    "        if \"unmanned\" not in t: \n",
    "            return \"Crewed\"\n",
    "\n",
    "    # 4. NOT APPLICABLE (Default for non-vehicles)\n",
    "    # If the system is clearly a component, weapon, or support item, it's N/A.\n",
    "    # We use a broad check for \"Not Applicable\" candidates.\n",
    "    na_indicators = [\n",
    "        \"missile\", \"munition\", \"bomb\", \"ammunition\", \"round\", \"cartridge\", \n",
    "        \"radar\", \"sensor\", \"radio\", \"software\", \"training\", \"simulator\", \n",
    "        \"engine\", \"spare part\", \"container\", \"gun\", \"artillery\", \"howitzer\",\n",
    "        \"maintenance\", \"support\", \"logistics\", \"service\"\n",
    "    ]\n",
    "    if any(k in t for k in na_indicators):\n",
    "        return \"Not Applicable\"\n",
    "        \n",
    "    # If specific system type is known from Stage 3 and it's not a platform\n",
    "    if system_type and system_type not in [\"Air Platform\", \"Naval Platform\", \"Land Platform\"]:\n",
    "         return \"Not Applicable\"\n",
    "\n",
    "    # Default fallback if ambiguous (often best to leave blank or N/A, but SOP says N/A for non-driving)\n",
    "    return \"Not Applicable\"\n",
    "\n",
    "\n",
    "def normalize_program_type_improved(pt_llm: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Improved Program Type logic with expanded keywords and better conflict resolution.\n",
    "    \"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    pt_llm = str(pt_llm).strip()\n",
    "\n",
    "    # 1. RDT&E (Strongest signal)\n",
    "    if any(k in text_lower for k in [\"rdt&e\", \"research\", \"development\", \"prototype\", \"demonstration\", \"sbir\", \"sttr\", \"study\", \"design phase\"]):\n",
    "        return \"RDT&E\"\n",
    "\n",
    "    # 2. MRO/Support (Strong signal)\n",
    "    # Added: \"sustainment\", \"cls\" (contractor logistics support), \"pbl\" (performance based logistics)\n",
    "    if any(k in text_lower for k in [\"mro\", \"sustainment\", \"maintenance\", \"repair\", \"overhaul\", \"logistics\", \"support services\", \"cls\", \"technical support\", \"engineering support\"]):\n",
    "        return \"MRO/Support\"\n",
    "\n",
    "    # 3. Upgrade (Specific action on existing)\n",
    "    if any(k in text_lower for k in [\"upgrade\", \"modernization\", \"retrofit\", \"modification\", \"life extension\", \"sle\", \"update\"]):\n",
    "        return \"Upgrade\"\n",
    "\n",
    "    # 4. Procurement (Broadest category, tricky vs Training)\n",
    "    # Explicit \"Procurement\" signals\n",
    "    procure_signals = [\"production\", \"delivery\", \"procurement\", \"acquisition\", \"supply\", \"purchase\", \"manufacture\", \"fabrication\", \"assembly\"]\n",
    "    if any(k in text_lower for k in procure_signals):\n",
    "        return \"Procurement\"\n",
    "\n",
    "    # Training HARDWARE check (Simulators -> Procurement)\n",
    "    if \"training\" in text_lower or \"simulator\" in text_lower:\n",
    "        if any(k in text_lower for k in [\"simulator\", \"device\", \"trainer\", \"hardware\", \"system\", \"kit\", \"aids\"]):\n",
    "             return \"Procurement\"\n",
    "    \n",
    "    # 5. Training (Services Only)\n",
    "    # If it says \"training\" and wasn't caught by Procurement above\n",
    "    if \"training\" in text_lower or \"instruction\" in text_lower:\n",
    "        return \"Training\"\n",
    "\n",
    "    # 6. Other Service (Catch-all for services)\n",
    "    if \"service\" in text_lower or \"labor\" in text_lower:\n",
    "        return \"Other Service\"\n",
    "\n",
    "    # Fallback: Trust LLM if it found a valid category\n",
    "    valid_types = [\"Procurement\", \"Training\", \"MRO/Support\", \"RDT&E\", \"Upgrade\", \"Other Service\"]\n",
    "    if pt_llm in valid_types:\n",
    "        return pt_llm\n",
    "\n",
    "    # Default if truly unknown but looks like a contract award\n",
    "    if \"award\" in text_lower:\n",
    "        return \"Procurement\" # Safe default for \"awarded contract for X\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def calculate_value_certainty_strict(text: str, value_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Highly recommends 'Confirmed'. Only 'Estimated' if explicitly stated.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # 1. Strong \"Confirmed\" Default\n",
    "    # If we extracted a specific non-zero value, assume Confirmed unless proven otherwise.\n",
    "    if not value_str or value_str == \"0.000\":\n",
    "        return \"Estimated\" # No value = Estimated/Unknown\n",
    "\n",
    "    # 2. Explicit Estimation Modifiers (Strict)\n",
    "    # Only tag 'Estimated' if these words explicitly modify the value context.\n",
    "    # \"Ceiling\" implies IDIQ max, which is an estimate of potential spend.\n",
    "    estimation_signals = [\"estimated value\", \"estimated cost\", \"ceiling\", \"maximum value\", \"potential value\", \"not to exceed\"]\n",
    "    \n",
    "    if any(k in text_lower for k in estimation_signals):\n",
    "        return \"Confirmed\"\n",
    "\n",
    "    # 3. IDIQ / BPA (Usually Ceiling = Estimated)\n",
    "    # However, if user wants \"Confirmed\" for the stated face value, we can be lenient.\n",
    "    # SOP says: \"Select Estimated when the value is not confirmed and possibility of future modifications.\"\n",
    "    # IDIQ ceilings ARE limits, not confirmed spend. So we keep them as Estimated.\n",
    "    if \"indefinite-delivery\" in text_lower or \"idiq\" in text_lower:\n",
    "         # Often IDIQs have a \"face value\" (initial task order) vs \"ceiling\".\n",
    "         # If the text says \"awarded a $X task order\", it's confirmed.\n",
    "         # If it says \"awarded a $X IDIQ contract\", it's a ceiling (Estimated).\n",
    "         if \"task order\" in text_lower or \"delivery order\" in text_lower:\n",
    "             return \"Estimated\" \n",
    "         return \"Confirmed\"\n",
    "\n",
    "    return \"Confirmed\"\n",
    "\n",
    "def extract_award_value_million_strict(paragraph: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract contract AWARD value strictly from award patterns only.\n",
    "    Avoids funding amounts later in the paragraph.\n",
    "    Output: string in million format like '12.015'\n",
    "    \"\"\"\n",
    "    t = str(paragraph)\n",
    "\n",
    "    patterns = [\n",
    "        r\"\\bis awarded a\\s+\\$([\\d,]+(?:\\.\\d+)?)\\b\",\n",
    "        r\"\\bare awarded an estimated aggregate ceiling of\\s+\\$([\\d,]+(?:\\.\\d+)?)\\b\",\n",
    "        r\"\\bare awarded an?\\s+\\$([\\d,]+(?:\\.\\d+)?)\\b\",\n",
    "        r\"\\bceiling of\\s+\\$([\\d,]+(?:\\.\\d+)?)\\b\",\n",
    "    ]\n",
    "\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                val = float(m.group(1).replace(\",\", \"\"))\n",
    "                return f\"{val/1_000_000:.3f}\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def detect_quantity_should_be_na(paragraph: str) -> bool:\n",
    "    \"\"\"\n",
    "    If paragraph contains many item quantities -> treat Quantity as Not Applicable.\n",
    "    Example3: 483 missiles, 82 missiles, 156 missiles, 198 containers...\n",
    "    That is NOT a single contract-level quantity.\n",
    "    \"\"\"\n",
    "    t = str(paragraph)\n",
    "\n",
    "    # Count numeric patterns that look like item quantities\n",
    "    qty_candidates = re.findall(r\"\\b(\\d{1,5})\\b\", t)\n",
    "\n",
    "    # If too many numbers appear, it's almost always a line-item contract\n",
    "    if len(qty_candidates) >= 12:\n",
    "        return True\n",
    "\n",
    "    # Specific strong markers for line-item heavy paragraphs\n",
    "    if any(k in t.lower() for k in [\"as follows:\", \"lot\", \"containers\", \"spare\", \"tail cap\", \"guidance unit\"]):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize_currency(cur: str) -> str:\n",
    "    if not cur:\n",
    "        return \"USD$\"\n",
    "    c = str(cur).strip().upper()\n",
    "    if c in [\"USD\", \"US$\", \"$\", \"US DOLLAR\", \"DOLLARS\"]:\n",
    "        return \"USD$\"\n",
    "    return c\n",
    "\n",
    "def word_to_int(token: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Converts word numbers to int:\n",
    "    eight -> 8, twenty one -> 21\n",
    "\n",
    "    Supports 0-99 safely.\n",
    "    Returns None if not a valid word-number.\n",
    "    \"\"\"\n",
    "    if not token:\n",
    "        return None\n",
    "\n",
    "    t = str(token).strip().lower()\n",
    "    t = t.replace(\"-\", \" \")\n",
    "    parts = [p for p in t.split() if p]\n",
    "\n",
    "    ones = {\n",
    "        \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "        \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "        \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14,\n",
    "        \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17, \"eighteen\": 18,\n",
    "        \"nineteen\": 19\n",
    "    }\n",
    "\n",
    "    tens = {\n",
    "        \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50,\n",
    "        \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90\n",
    "    }\n",
    "\n",
    "    # single word\n",
    "    if len(parts) == 1:\n",
    "        if parts[0] in ones:\n",
    "            return ones[parts[0]]\n",
    "        if parts[0] in tens:\n",
    "            return tens[parts[0]]\n",
    "        return None\n",
    "\n",
    "    # two words: \"twenty one\"\n",
    "    if len(parts) == 2:\n",
    "        if parts[0] in tens and parts[1] in ones:\n",
    "            return tens[parts[0]] + ones[parts[1]]\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_qty_token(qty_token: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Converts numeric token or word-number token into int.\n",
    "    \"\"\"\n",
    "    if qty_token is None:\n",
    "        return None\n",
    "    s = str(qty_token).strip().lower()\n",
    "\n",
    "    # numeric\n",
    "    if re.fullmatch(r\"\\d+\", s):\n",
    "        return int(s)\n",
    "\n",
    "    # word-number\n",
    "    return word_to_int(s)\n",
    "\n",
    "\n",
    "def parse_fms_countries(paragraph: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract FMS customer countries list.\n",
    "    Example: 'governments of Australia, Bahrain, Belgium...'\n",
    "    \"\"\"\n",
    "    text = str(paragraph)\n",
    "\n",
    "    m = re.search(\n",
    "        r\"governments of (.+?)(?:\\.\\s| Work will be performed| Fiscal| This contract|$)\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if not m:\n",
    "        return []\n",
    "\n",
    "    block = m.group(1)\n",
    "    raw = re.split(r\",|\\band\\b\", block)\n",
    "\n",
    "    countries = []\n",
    "    for c in raw:\n",
    "        c = c.strip()\n",
    "        if 2 < len(c) <= 40:\n",
    "            countries.append(c)\n",
    "\n",
    "    final, seen = [], set()\n",
    "    for c in countries:\n",
    "        if c.lower() not in seen:\n",
    "            final.append(c)\n",
    "            seen.add(c.lower())\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def detect_multiple_supplier_award(paragraph: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect Example2 pattern:\n",
    "    Many suppliers listed with semicolons + \"are awarded ...\"\n",
    "    \"\"\"\n",
    "    t = str(paragraph).strip()\n",
    "    if \";\" in t and re.search(r\"\\bare awarded\\b\", t, flags=re.IGNORECASE):\n",
    "        if re.search(r\"\\([A-Z0-9]{6,}\\)\", t):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def parse_line_item_operator_allocations(paragraph: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract line-item splits like:\n",
    "\n",
    "    '483 AIM-9X ... missiles (212 for the Navy, 187 for the Air Force and 84 for FMS customers);'\n",
    "    '82 AIM-9X ... missiles (eight for the Navy, eight for the Air Force and 66 for FMS customers);'\n",
    "\n",
    "    Output:\n",
    "      [\n",
    "        {\"item_name\": \"...\", \"operator\":\"Navy\", \"quantity\":\"212\", \"g2g_b2g\":\"B2G\"},\n",
    "        {\"item_name\": \"...\", \"operator\":\"Air Force\", \"quantity\":\"187\", \"g2g_b2g\":\"B2G\"},\n",
    "        {\"item_name\": \"...\", \"operator\":\"Foreign Assistance\", \"quantity\":\"84\", \"g2g_b2g\":\"G2G\"}\n",
    "      ]\n",
    "    \"\"\"\n",
    "    text = str(paragraph)\n",
    "\n",
    "    # This finds: <total qty> <item desc> ( <allocation block> )\n",
    "    # Example: 483 AIM-9X ... missiles (212 for the Navy, 187 for the Air Force and 84 for FMS customers)\n",
    "    item_pattern = r\"\\b(\\d+)\\s+(.+?)\\s*\\(([^)]*for[^)]*)\\)\"\n",
    "    matches = re.findall(item_pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for total_qty, item_desc, alloc_block in matches:\n",
    "        item_desc_clean = re.sub(r\"\\s+\", \" \", item_desc).strip(\" ;,. -\")\n",
    "\n",
    "        # --- BRANCH allocations (digit or word-number)\n",
    "        branch_pattern = r\"\\b(\\d+|[A-Za-z\\-]+)\\s+for\\s+the\\s+(Navy|Air Force|Army|Marine Corps|Space Force|Coast Guard|Defense Logistics Agency|Missile Defense Agency)\\b\"\n",
    "        for qty_token, op_raw in re.findall(branch_pattern, alloc_block, flags=re.IGNORECASE):\n",
    "            qty_val = parse_qty_token(qty_token)\n",
    "            if qty_val is None:\n",
    "                continue\n",
    "            normalized_op = normalize_customer_operator(op_raw)       \n",
    "            results.append({\n",
    "                \"item_name\": item_desc_clean,\n",
    "                \"operator\": normalized_op,\n",
    "                \"quantity\": str(qty_val),\n",
    "                \"g2g_b2g\": \"B2G\"\n",
    "            })\n",
    "\n",
    "        # --- FMS allocations (digit or word-number)\n",
    "        fms_pattern = r\"\\b(\\d+|[A-Za-z\\-]+)\\s+for\\s+(?:Foreign Military Sales\\s*\\(FMS\\)\\s*customers|Foreign Military Sales\\s*customers|FMS\\s*customers|FMS)\\b\"\n",
    "        for qty_token in re.findall(fms_pattern, alloc_block, flags=re.IGNORECASE):\n",
    "            qty_val = parse_qty_token(qty_token)\n",
    "            if qty_val is None:\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"item_name\": item_desc_clean,\n",
    "                \"operator\": \"Foreign Assistance\",\n",
    "                \"quantity\": str(qty_val),\n",
    "                \"g2g_b2g\": \"G2G\"\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def split_rows_engine(base_row: dict, paragraph: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    FINAL STAGE5 Split Engine (Your required behavior)\n",
    "\n",
    "    SPLITS:\n",
    "    1) Multi-supplier award => Supplier Name = Multiple (NO supplier split)\n",
    "    2) Line-item + operator allocation split (Navy/AirForce/FMS)\n",
    "    3) FMS country split ONLY for G2G rows\n",
    "\n",
    "    Only split fields are modified, rest remain unchanged.\n",
    "    \"\"\"\n",
    "    paragraph = str(paragraph).strip()\n",
    "    row = base_row.copy()\n",
    "\n",
    "    split_reasons = []\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) MULTIPLE SUPPLIER AWARD (Example2)\n",
    "    # ------------------------------------------------------------------\n",
    "    if detect_multiple_supplier_award(paragraph):\n",
    "        row[\"Supplier Name\"] = \"Multiple\"\n",
    "        row[\"Supplier Name Evidence\"] = \"Multiple Suppliers (Detected)\"\n",
    "        row[\"Split Flag\"] = \"No\"\n",
    "        row[\"Split Reason\"] = \"Multiple supplier award detected (no supplier split)\"\n",
    "        return [row]\n",
    "\n",
    "    rows = [row]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) LINE-ITEM + OPERATOR SPLIT (Your expected output)\n",
    "    # ------------------------------------------------------------------\n",
    "    item_allocs = parse_line_item_operator_allocations(paragraph)\n",
    "\n",
    "    if item_allocs:\n",
    "        split_reasons.append(\"Line-item operator allocation split\")\n",
    "\n",
    "        new_rows = []\n",
    "        for r in rows:\n",
    "            for alloc in item_allocs:\n",
    "                rr = r.copy()\n",
    "\n",
    "                # Only split fields change\n",
    "                rr[\"Customer Operator\"] = alloc[\"operator\"]\n",
    "                rr[\"Quantity\"] = alloc[\"quantity\"]\n",
    "                rr[\"G2G/B2G\"] = alloc[\"g2g_b2g\"]\n",
    "\n",
    "                # Make system labels reflect line-item name (matches your expected output)\n",
    "                rr[\"System Name (General)\"] = alloc[\"item_name\"]\n",
    "                rr[\"System Name (Specific)\"] = alloc[\"item_name\"]\n",
    "\n",
    "                new_rows.append(rr)\n",
    "\n",
    "        rows = new_rows\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) FMS COUNTRY SPLIT (ONLY for G2G rows)\n",
    "    # ------------------------------------------------------------------\n",
    "    fms_countries = parse_fms_countries(paragraph)\n",
    "    if fms_countries:\n",
    "        split_reasons.append(\"FMS country split\")\n",
    "\n",
    "        final_rows = []\n",
    "        for r in rows:\n",
    "            is_g2g = str(r.get(\"G2G/B2G\", \"\")).strip().upper() == \"G2G\"\n",
    "            is_fms_operator = \"foreign\" in str(r.get(\"Customer Operator\", \"\")).lower()\n",
    "\n",
    "            if is_g2g or is_fms_operator:\n",
    "                for c in fms_countries:\n",
    "                    rr = r.copy()\n",
    "                    rr[\"Customer Country\"] = normalize_country_name(c)\n",
    "                    rr[\"Customer Region\"] = get_region_for_country(rr[\"Customer Country\"])\n",
    "                    final_rows.append(rr)\n",
    "            else:\n",
    "                final_rows.append(r)\n",
    "\n",
    "        rows = final_rows\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Final flags\n",
    "    # ------------------------------------------------------------------\n",
    "    if split_reasons:\n",
    "        reason = \" | \".join(split_reasons)\n",
    "        for r in rows:\n",
    "            r[\"Split Flag\"] = \"Yes\"\n",
    "            r[\"Split Reason\"] = reason\n",
    "    else:\n",
    "        for r in rows:\n",
    "            r[\"Split Flag\"] = \"No\"\n",
    "            r[\"Split Reason\"] = \"No split condition found\"\n",
    "\n",
    "    return rows\n",
    "\n",
    "## LLM CALL HELPER (OpenRouter Safe Wrapper)\n",
    "def call_llm_json(system_prompt: str, user_prompt: str, max_tokens: int):\n",
    "    \"\"\"\n",
    "    Safe OpenRouter call wrapper\n",
    "    - JSON response enforced\n",
    "    - max_tokens enforced\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=OPENROUTER_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "# 11) EXCEL HIGHLIGHTING FEATURE\n",
    "\n",
    "def highlight_evidence_reason_columns(excel_path: str):\n",
    "    wb = load_workbook(excel_path)\n",
    "\n",
    "    # Always ensure at least one visible sheet\n",
    "    ws = wb.active\n",
    "    ws.sheet_state = \"visible\"\n",
    "\n",
    "    header = [cell.value for cell in ws[1]]\n",
    "\n",
    "    evidence_cols = []\n",
    "    reason_cols = []\n",
    "\n",
    "    for idx, col_name in enumerate(header, start=1):\n",
    "        if isinstance(col_name, str) and \"Evidence\" in col_name:\n",
    "            evidence_cols.append(idx)\n",
    "        if isinstance(col_name, str) and \"Reason\" in col_name:\n",
    "            reason_cols.append(idx)\n",
    "\n",
    "    evidence_fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
    "    reason_fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "    header_font = Font(bold=True)\n",
    "\n",
    "    for col_idx in evidence_cols:\n",
    "        ws.cell(row=1, column=col_idx).fill = evidence_fill\n",
    "        ws.cell(row=1, column=col_idx).font = header_font\n",
    "\n",
    "    for col_idx in reason_cols:\n",
    "        ws.cell(row=1, column=col_idx).fill = reason_fill\n",
    "        ws.cell(row=1, column=col_idx).font = header_font\n",
    "\n",
    "    for row in range(2, ws.max_row + 1):\n",
    "        for col_idx in evidence_cols:\n",
    "            ws.cell(row=row, column=col_idx).fill = evidence_fill\n",
    "        for col_idx in reason_cols:\n",
    "            ws.cell(row=row, column=col_idx).fill = reason_fill\n",
    "\n",
    "    wb.save(excel_path)\n",
    "    print(\"Evidence + Reason columns highlighted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1facc0a",
   "metadata": {},
   "source": [
    "**Using Knowledgebase to support my extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46ef5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG RETRIEVER (FAISS + METADATA)\n",
    "class SystemKBRetriever:\n",
    "    def __init__(self, kb_dir: str, embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.kb_dir = kb_dir\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "        index_path = os.path.join(kb_dir, \"system_kb.faiss\")\n",
    "        meta_path = os.path.join(kb_dir, \"system_kb_meta.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_path) or not os.path.exists(meta_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"KB files not found in: {kb_dir}\\n\"\n",
    "                f\"Expected:\\n- {index_path}\\n- {meta_path}\\n\\n\"\n",
    "                f\"Build KB first using your KB builder script.\"\n",
    "            )\n",
    "        print(f\"Loading FAISS Index: {index_path}\")\n",
    "        self.index = faiss.read_index(index_path)\n",
    "\n",
    "        print(f\"Loading KB Metadata: {meta_path}\")\n",
    "        with open(meta_path, \"rb\") as f:\n",
    "            self.meta = pickle.load(f)\n",
    "\n",
    "        print(f\"KB Loaded rows: {len(self.meta)}\")\n",
    "\n",
    "        self.embedder = None\n",
    "\n",
    "    def _lazy_load_embedder(self):\n",
    "        if self.embedder is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.embedder = SentenceTransformer(self.embed_model)\n",
    "\n",
    "    def retrieve(self, query_text: str, top_k: int = 3):\n",
    "        import numpy as np\n",
    "        query_text = str(query_text).strip()\n",
    "        if not query_text:\n",
    "            return []\n",
    "\n",
    "        self._lazy_load_embedder()\n",
    "\n",
    "        q_emb = self.embedder.encode([query_text], normalize_embeddings=True).astype(\"float32\")\n",
    "        scores, idxs = self.index.search(q_emb, top_k)\n",
    "\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "            results.append({\"score\": float(score), \"meta\": self.meta[idx]})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2df4e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS Index: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\system_kb_store\\system_kb.faiss\n",
      "Loading KB Metadata: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\system_kb_store\\system_kb_meta.pkl\n",
      "KB Loaded rows: 600\n"
     ]
    }
   ],
   "source": [
    "retriever = SystemKBRetriever(kb_dir=RAG_KB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8198e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_str(x, default=\"\"):\n",
    "    \"\"\"\n",
    "    Safely converts input to string, returning default if None or empty/whitespace.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return default\n",
    "    s = str(x).strip()\n",
    "    return s if s else default\n",
    "\n",
    "def rag_best_hit(paragraph: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Retrieves the single best match from the Knowledge Base (FAISS).\n",
    "    Returns: (hit_object, score, metadata_dict)\n",
    "    \"\"\"\n",
    "    # Ensure the global retriever is loaded\n",
    "    if 'retriever' not in globals():\n",
    "        print(\"Warning: 'retriever' is not defined. Returning empty hit.\")\n",
    "        return None, 0.0, {}\n",
    "\n",
    "    hits = retriever.retrieve(paragraph, top_k=top_k)\n",
    "    \n",
    "    if not hits:\n",
    "        return None, 0.0, {}\n",
    "        \n",
    "    best = hits[0]\n",
    "    return best, float(best.get(\"score\", 0.0)), best.get(\"meta\", {})\n",
    "\n",
    "#-------------------------------------------\n",
    "RAG_STRONG_THRESHOLD = 0.78   # if >= this, trust KB fully\n",
    "RAG_MEDIUM_THRESHOLD = 0.70   # if >= this, use KB as strong hint\n",
    "\n",
    "def normalize_country_name(c: str) -> str:\n",
    "    if not c:\n",
    "        return \"Unknown\"\n",
    "    t = str(c).strip().lower()\n",
    "    if t in [\"united states\", \"united states of america\", \"us\", \"u.s.\", \"usa\", \"america\"]:\n",
    "        return \"USA\"\n",
    "    if t in [\"united kingdom\", \"uk\", \"u.k.\", \"britain\", \"great britain\"]:\n",
    "        return \"UK\"\n",
    "    return str(c).strip()\n",
    "\n",
    "def rag_best_hit(paragraph: str, top_k: int = 3):\n",
    "    hits = retriever.retrieve(paragraph, top_k=top_k)\n",
    "    if not hits:\n",
    "        return None, 0.0, {}\n",
    "    best = hits[0]\n",
    "    return best, float(best.get(\"score\", 0.0)), best.get(\"meta\", {})\n",
    "\n",
    "def safe_str(x, default=\"\"):\n",
    "    if x is None:\n",
    "        return default\n",
    "    s = str(x).strip()\n",
    "    return s if s else default\n",
    "\n",
    "def normalize_program_type_strict(pt_llm: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Decides Program Type based on strict SOP hierarchy.\n",
    "    Priority: RDT&E > MRO > Upgrade > Procurement (Hardware/Simulators) > Training (Services).\n",
    "    \"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    pt_llm = str(pt_llm).strip()\n",
    "\n",
    "    # 1. RDT&E (Highest Priority)\n",
    "    # Detects: Prototypes, Research, Development, SBIR\n",
    "    if any(k in text_lower for k in [\"rdt&e\", \"research\", \"development\", \"prototype\", \"sbir\", \"demonstration\"]):\n",
    "        return \"RDT&E\"\n",
    "\n",
    "    # 2. MRO/Support\n",
    "    # Detects: Sustainment, Maintenance, Logistics, Repair, Overhaul\n",
    "    if any(k in text_lower for k in [\"mro\", \"sustainment\", \"maintenance\", \"logistics\", \"repair\", \"overhaul\", \"support services\"]):\n",
    "        return \"MRO/Support\"\n",
    "\n",
    "    # 3. Upgrade\n",
    "    # Detects: Modernization, Retrofit, Modification of existing platforms\n",
    "    if any(k in text_lower for k in [\"upgrade\", \"modernization\", \"retrofit\", \"modification\", \"life extension\"]):\n",
    "        return \"Upgrade\"\n",
    "\n",
    "    # 4. Training (Strict Split)\n",
    "    # Rule: Training SERVICES = Training.\n",
    "    # Rule: Training HARDWARE (Simulators, Aircraft) = Procurement.\n",
    "    if \"training\" in text_lower or \"training\" in pt_llm.lower():\n",
    "        # Check for hardware indicators\n",
    "        if any(k in text_lower for k in [\"simulator\", \"device\", \"trainer aircraft\", \"hardware\", \"system\", \"kit\"]):\n",
    "            return \"Procurement\"\n",
    "        return \"Training\"\n",
    "\n",
    "    # 5. Procurement\n",
    "    # Detects: Production, Delivery, Acquisition, Construction\n",
    "    if any(k in text_lower for k in [\"production\", \"delivery\", \"procurement\", \"acquisition\", \"supply\", \"purchase\"]):\n",
    "        return \"Procurement\"\n",
    "\n",
    "    # Fallback to LLM's choice if valid, otherwise Other\n",
    "    if pt_llm in [\"Procurement\", \"Training\", \"MRO/Support\", \"RDT&E\", \"Upgrade\", \"Other Service\"]:\n",
    "        return pt_llm\n",
    "\n",
    "    return \"Other Service\"\n",
    "\n",
    "\n",
    "def calculate_value_certainty(text: str, value_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Determines if value is 'Confirmed' or 'Estimated'.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # 1. Keywords indicating estimation/limit\n",
    "    if any(k in text_lower for k in [\"ceiling\", \"maximum\", \"estimated\", \"potential\", \"not to exceed\", \"idiq\", \"bpa\", \"blanket purchase agreement\"]):\n",
    "        return \"Estimated\"\n",
    "\n",
    "    # 2. Shared/Split Value Indicators\n",
    "    # If multiple companies share a pot, the value is Estimated for that specific row\n",
    "    if any(k in text_lower for k in [\"shared\", \"competing\", \"each awarded\", \"multiple award\", \"aggregate\"]):\n",
    "        return \"Estimated\"\n",
    "\n",
    "    # 3. If no value was extracted, it can't be confirmed\n",
    "    if not value_str or value_str == \"0.000\":\n",
    "        return \"Estimated\"\n",
    "\n",
    "    return \"Confirmed\"\n",
    "\n",
    "\n",
    "def calculate_usd_value(val_million_str: str, currency: str) -> str:\n",
    "    \"\"\"\n",
    "    Populates 'Value (USD$ Million)'.\n",
    "    Assuming 1:1 if already USD, otherwise leaves blank or requires conversion logic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        val = float(val_million_str)\n",
    "        if val == 0:\n",
    "            return \"\"\n",
    "            \n",
    "        # If Currency is USD, copy the value\n",
    "        if \"USD\" in currency.upper() or \"$\" in currency:\n",
    "            return f\"{val:.3f}\"\n",
    "            \n",
    "        # (Optional) Add simplistic conversion if needed, e.g., GBP -> 1.25 * Val\n",
    "        # For now, we return extraction.\n",
    "        return f\"{val:.3f}\"\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb994b",
   "metadata": {},
   "source": [
    "## AGENTS / TOOLS (Chunk-wise extraction + Merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06857a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENTS / TOOLS (Chunk-wise extraction + Merge)\n",
    "\n",
    "# Stage 1: SOURCING EXTRACTOR\n",
    "\n",
    "class SourcingInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Full contract paragraph/description text.\")\n",
    "    url: str = Field(description=\"Source URL of the contract announcement/news.\")\n",
    "    date: str = Field(description=\"Contract date in Excel (string).\")\n",
    "\n",
    "@tool(\"sourcing_extractor\")\n",
    "def sourcing_extractor(paragraph: str, url: str, date: str):\n",
    "    \"\"\"\n",
    "    Stage 1: SOURCING EXTRACTOR\n",
    "\n",
    "    Purpose:\n",
    "    - Creates the base skeleton row (stable fields).\n",
    "\n",
    "    Output columns created:\n",
    "    - Description of Contract\n",
    "    - Additional Notes (Internal Only)\n",
    "    - Source Link(s)\n",
    "    - Contract Date\n",
    "    - Reported Date (By SGA)\n",
    "\n",
    "    Important:\n",
    "    - These fields remain SAME even after splits.\n",
    "    - Every split row inherits these values.\n",
    "    \"\"\"\n",
    "    reported_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    notes = \"Standard extraction.\"\n",
    "    if \"modification\" in str(paragraph).lower():\n",
    "        notes = \"Contract Modification.\"\n",
    "    if \"multiple award\" in str(paragraph).lower():\n",
    "        notes = \"Multiple award contract detected (non-supplier split).\"\n",
    "\n",
    "    return {\n",
    "        \"Description of Contract\": paragraph,\n",
    "        \"Additional Notes (Internal Only)\": notes,\n",
    "        \"Source Link(s)\": url,\n",
    "        \"Contract Date\": date,\n",
    "        \"Reported Date (By SGA)\": reported_date\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73d68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## # Stage 2: GEOGRAPHY EXTRACTOR (Chunk -> Merge)\n",
    "\n",
    "class GeographyInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Full contract paragraph/description text.\")\n",
    "\n",
    "@tool(\"geography_extractor\")\n",
    "def geography_extractor(paragraph: str):\n",
    "    \"\"\"\n",
    "    Stage 2: GEOGRAPHY EXTRACTOR\n",
    "    Updates Supplier Region based on the extracted Supplier Country.\n",
    "    \"\"\"\n",
    "    paragraph = str(paragraph).strip()\n",
    "    if not paragraph:\n",
    "        return {}\n",
    "\n",
    "    # --- RAG Optimization ---\n",
    "    best_hit, best_score, best_meta = rag_best_hit(paragraph, top_k=3)\n",
    "    \n",
    "    # Defaults\n",
    "    cust_country = \"Unknown\"\n",
    "    cust_op = \"Unknown\"\n",
    "    supp_country = \"Unknown\"\n",
    "\n",
    "    if best_hit and best_score >= 0.78:\n",
    "        # Trust KB if high confidence\n",
    "        cust_country = safe_str(best_meta.get(\"Customer Country\", \"\"))\n",
    "        cust_op = normalize_customer_operator(safe_str(best_meta.get(\"Customer Operator\", \"\")))\n",
    "        supp_country = safe_str(best_meta.get(\"Supplier Country\", \"\"))\n",
    "    else:\n",
    "        # LLM Extraction\n",
    "        chunks = chunk_text(paragraph, chunk_size=1800, overlap=250)\n",
    "        outputs = []\n",
    "        for ch in chunks:\n",
    "            try:\n",
    "                raw = call_llm_json(GEOGRAPHY_PROMPT, ch, max_tokens=250)\n",
    "                outputs.append(raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        cust_country = pick_best_non_empty([o.get(\"Customer Country\") for o in outputs]) or \"Unknown\"\n",
    "        cust_op = normalize_customer_operator(pick_best_non_empty([o.get(\"Customer Operator\") for o in outputs]))\n",
    "        supp_country = pick_best_non_empty([o.get(\"Supplier Country\") for o in outputs]) or \"Unknown\"\n",
    "\n",
    "    # Normalize Countries\n",
    "    cust_country = normalize_country_name(cust_country)\n",
    "    supp_country = normalize_country_name(supp_country)\n",
    "\n",
    "    # Calculate Regions using the SHARED logic\n",
    "    cust_region = get_region_for_country(cust_country)\n",
    "    supp_region = get_region_for_country(supp_country) # Same list as Customer Region\n",
    "\n",
    "    return {\n",
    "        \"Customer Region\": cust_region,\n",
    "        \"Customer Country\": cust_country,\n",
    "        \"Customer Operator\": cust_op,\n",
    "        \"Supplier Region\": supp_region,   # Extracted here\n",
    "        \"Supplier Country\": supp_country,\n",
    "        \"Domestic Content\": \"Indigenous\" if cust_country == supp_country else \"Imported\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b3317ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: SYSTEM CLASSIFIER (Chunk -> Merge)\n",
    "\n",
    "class SystemInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Full contract paragraph/description text.\")\n",
    "    item_focus: str = Field(description=\"Specific line-item focus (from Stage5), e.g. 'All up round containers'\")\n",
    "\n",
    "@tool(\"system_classifier\")\n",
    "def system_classifier(paragraph: str, item_focus: str = \"\"):\n",
    "    \"\"\"\n",
    "    Stage 3: SYSTEM CLASSIFIER (Updated with Strict Piloting)\n",
    "    \"\"\"\n",
    "    paragraph = str(paragraph).strip()\n",
    "    \n",
    "    # RAG & LLM Logic (Same as before)\n",
    "    rag_query = f\"{item_focus} {paragraph}\" if item_focus else paragraph\n",
    "    rag_hits = retriever.retrieve(rag_query, top_k=3)\n",
    "    rag_context = json.dumps([h['meta'] for h in rag_hits if h.get('score', 0) > 0.6], indent=2)\n",
    "    \n",
    "    rule_hint = \"No specific overrides.\"\n",
    "    if \"container\" in item_focus.lower(): rule_hint = \"Classify as Support Equipment / Containers.\"\n",
    "\n",
    "    formatted_system_prompt = SYSTEM_CLASSIFIER_PROMPT.format(\n",
    "        taxonomy_reference=TAXONOMY_STR,\n",
    "        rule_book_overrides=rule_hint\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    ITEM_FOCUS: {item_focus if item_focus else \"Main System\"}\n",
    "    TEXT: {paragraph}\n",
    "    SIMILAR RAG EXAMPLES: {rag_context}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        res = call_llm_json(formatted_system_prompt, user_prompt, max_tokens=850)\n",
    "        \n",
    "        # Apply Improved Piloting Logic\n",
    "        designators = extract_designators(paragraph)\n",
    "        # Pass the extracted system type to help decide \"Not Applicable\"\n",
    "        sys_type = res.get(\"System Type (General)\", \"\")\n",
    "        res[\"System Piloting\"] = detect_piloting_strict(paragraph, designators, sys_type)\n",
    "        \n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return {\"Confidence\": \"Low\", \"Error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: CONTRACT EXTRACTOR (Chunk -> Merge + Strict Supplier)\n",
    "\n",
    "class ContractInfoInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Full contract paragraph/description text.\")\n",
    "    contract_date: str = Field(description=\"Contract date as string.\")\n",
    "\n",
    "@tool(\"contract_extractor\")\n",
    "def contract_extractor(paragraph: str, contract_date: str):\n",
    "    \"\"\"\n",
    "    Stage 4: CONTRACT EXTRACTOR (Updated to use LLM for Value Extraction)\n",
    "    \"\"\"\n",
    "    paragraph = str(paragraph).strip()\n",
    "\n",
    "    # 1. Regex Supplier (Keep this, regex is good for Proper Nouns)\n",
    "    supplier_name, supplier_evidence = extract_awardee_supplier_strict(paragraph)\n",
    "\n",
    "    # 2. Define a specialized prompt for Value extraction that \"understands\" context\n",
    "    # We ask the LLM to identify the AWARD value specifically, distinct from obligations.\n",
    "    value_extraction_prompt = \"\"\"\n",
    "    You are a defense contract analyst. Extract the following from the text:\n",
    "    1. \"value_million\": The total face value of the contract action in MILLIONS (float).\n",
    "       - Ignore \"obligated\" amounts or \"fiscal funds\" unless they are the ONLY value present.\n",
    "       - If the text says \"modification... in the amount of $X\", use X.\n",
    "       - If the text says \"ceiling of $Y\", use Y.\n",
    "    2. \"currency\": The currency code (default USD).\n",
    "    3. \"program_type\": Classify as Procurement, Training, MRO/Support, RDT&E, Upgrade, or Other.\n",
    "    4. \"g2g_b2g\": \"G2G\" if Foreign Military Sales (FMS), otherwise \"B2G\".\n",
    "\n",
    "    Return JSON only.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"CONTRACT TEXT: {paragraph}\\nDATE: {contract_date}\"\n",
    "\n",
    "    try:\n",
    "        # Re-using your existing call_llm_json helper\n",
    "        llm_data = call_llm_json(value_extraction_prompt, user_prompt, max_tokens=350)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Error: {e}\")\n",
    "        llm_data = {}\n",
    "\n",
    "    # 3. Extract Value from LLM response (Fallback to 0.000)\n",
    "    val_float = llm_data.get(\"value_million\", 0.0)\n",
    "    # Ensure it's a string with 3 decimal places for consistency\n",
    "    try:\n",
    "        val_formatted = f\"{float(val_float):.3f}\"\n",
    "    except:\n",
    "        val_formatted = \"0.000\"\n",
    "\n",
    "    # 4. Apply Logic (Program Type, Currency, etc.)\n",
    "    llm_pt = llm_data.get(\"program_type\", \"\")\n",
    "    final_program_type = normalize_program_type_improved(llm_pt, paragraph)\n",
    "    \n",
    "    # Use your strict certainty logic, but now applied to the LLM-found value\n",
    "    final_certainty = calculate_value_certainty_strict(paragraph, val_formatted)\n",
    "    currency = normalize_currency(llm_data.get(\"currency\", \"USD\"))\n",
    "    usd_val_million = calculate_usd_value(val_formatted, currency)\n",
    "    mro_months = calculate_mro_months(contract_date, llm_data.get(\"completion_date_text\", \"\"), final_program_type)\n",
    "\n",
    "    return {\n",
    "        \"Supplier Name\": supplier_name,\n",
    "        \"Supplier Name Evidence\": supplier_evidence,\n",
    "        \"Program Type\": final_program_type,\n",
    "        \"Value Certainty\": final_certainty,\n",
    "        \"Value (Million)\": val_formatted,      # Now populated by LLM understanding\n",
    "        \"Value (USD$ Million)\": usd_val_million,\n",
    "        \"Currency\": currency,\n",
    "        \"Expected MRO Contract Duration (Months)\": mro_months,\n",
    "        \"Quantity\": \"Not Applicable\",\n",
    "        \"G2G/B2G\": llm_data.get(\"g2g_b2g\", \"B2G\"),\n",
    "        \"Value Note (If Any)\": llm_data.get(\"value_note\", \"\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90a66e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5: SPLITTER AGENT (FULL PARAGRAPH ALWAYS)\n",
    "\n",
    "class SplitterInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Full contract paragraph/description text.\")\n",
    "    base_row: dict = Field(description=\"Extracted row after Stage1-4.\")\n",
    "\n",
    "@tool(\"splitter_agent\")\n",
    "def splitter_agent(paragraph: str, base_row: dict):\n",
    "  \"\"\"\n",
    "  Stage 5: SPLITTER AGENT\n",
    "  Purpose:\n",
    "    - Applies deterministic split logic to generate multiple output rows\n",
    "      when paragraph has explicit multi allocations.\n",
    "  Supported splits:\n",
    "    - Operator/Quantity split (\"212 for the Navy\", \"187 for the Air Force\")\n",
    "    - FMS country split (only for rows marked as G2G)\n",
    "    - Multi-value note (does not split, only notes)\n",
    "\n",
    "  IMPORTANT:\n",
    "    - Supplier split is REMOVED to prevent wrong supplier explosions.\n",
    "  \"\"\"\n",
    "  try:\n",
    "      rows = split_rows_engine(base_row, paragraph)\n",
    "      for r in rows:\n",
    "          r.setdefault(\"Split Flag\", \"No\")\n",
    "          r.setdefault(\"Split Reason\", \"\")\n",
    "      return {\"rows\": rows}\n",
    "  except Exception as e:\n",
    "      base_row[\"Split Flag\"] = \"Error\"\n",
    "      base_row[\"Split Reason\"] = f\"Split failed: {str(e)}\"\n",
    "      return {\"rows\": [base_row]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a858a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 6: QUALITY VALIDATOR AGENT\n",
    "\n",
    "class QAInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Original paragraph for reference.\")\n",
    "    rows: list = Field(description=\"Final split rows list output from Stage5.\")\n",
    "\n",
    "@tool(\"quality_validator\")\n",
    "def quality_validator(paragraph: str, rows: list):\n",
    "    \"\"\"\n",
    "    Stage 6: QUALITY VALIDATOR (Hybrid: Rule-Based + KB Check)\n",
    "    \"\"\"\n",
    "    text = str(paragraph).lower()\n",
    "    validated_rows = []\n",
    "\n",
    "    for r in rows:\n",
    "        flags = []\n",
    "        fixes = []\n",
    "\n",
    "        # --- Rule Based Checks ---\n",
    "        supplier = str(r.get(\"Supplier Name\", \"\")).strip()\n",
    "        if supplier.lower() in [\"unknown\", \"\", \"multiple\"]:\n",
    "             # If text clearly has an award, flag it\n",
    "             if \"awarded\" in text:\n",
    "                 flags.append(\"Supplier Unknown\")\n",
    "                 fixes.append(\"Check LLM Fallback\")\n",
    "\n",
    "        # --- KB/Taxonomy Validation (NEW) ---\n",
    "        sys_name = str(r.get(\"System Name (General)\", \"\")).strip()\n",
    "        \n",
    "        # If we have a system name, verify it against KB context\n",
    "        if sys_name and sys_name.lower() != \"unknown\":\n",
    "            # Quick check: does this system appear in our RAG hits?\n",
    "            # We assume 'retriever' is globally available as per notebook scope\n",
    "            hits = retriever.retrieve(sys_name, top_k=1)\n",
    "            if hits:\n",
    "                top_score = hits[0]['score']\n",
    "                # If score is very low, the system might be hallucinated or poorly named\n",
    "                if top_score < 0.35: \n",
    "                    flags.append(f\"System Name '{sys_name}' has low KB confidence ({top_score:.2f})\")\n",
    "                    fixes.append(\"Verify system name against standard taxonomy\")\n",
    "\n",
    "        # --- Final Status ---\n",
    "        qa_status = \"PASS\" if not flags else \"FAIL\"\n",
    "        \n",
    "        rr = r.copy()\n",
    "        rr[\"QA Status\"] = qa_status\n",
    "        rr[\"QA Flags\"] = \" | \".join(flags) if flags else \"None\"\n",
    "        rr[\"QA Fix Suggestion\"] = \" | \".join(fixes) if fixes else \"None\"\n",
    "        validated_rows.append(rr)\n",
    "\n",
    "    return {\"rows\": validated_rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53cf7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 7: LLM VALIDATOR (FAIL ONLY) - Chunk Aware\n",
    "class LLMValidateInput(BaseModel):\n",
    "    paragraph: str = Field(description=\"Original paragraph text.\")\n",
    "    row: dict = Field(description=\"One FAIL row to validate/correct.\")\n",
    "\n",
    "@tool(\"llm_fail_row_validator\")\n",
    "def llm_fail_row_validator(paragraph: str, row: dict):\n",
    "    \"\"\"\n",
    "    Stage 7: VALIDATOR FIX (Linked to prompts.py)\n",
    "    \"\"\"\n",
    "    # LINK THE PROMPT\n",
    "    formatted_fix_prompt = VALIDATOR_FIX_PROMPT.format(\n",
    "        failed_row_json=json.dumps(row, indent=2),\n",
    "        program_type_enum=str(PROGRAM_TYPE_ALLOWED)\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"ORIGINAL TEXT CHUNK: {paragraph[:2000]}\"\n",
    "\n",
    "    try:\n",
    "        fix = call_llm_json(formatted_fix_prompt, user_prompt, max_tokens=350)\n",
    "        \n",
    "        # Merge Fixes\n",
    "        corrected = row.copy()\n",
    "        if fix.get(\"Supplier Name\") and fix[\"Supplier Name\"] != \"Unknown\":\n",
    "            corrected[\"Supplier Name\"] = fix[\"Supplier Name\"]\n",
    "        if fix.get(\"Program Type\"):\n",
    "            corrected[\"Program Type\"] = fix[\"Program Type\"]\n",
    "        if fix.get(\"Value (Million)\"):\n",
    "            corrected[\"Value (Million)\"] = fix[\"Value (Million)\"]\n",
    "            \n",
    "        corrected[\"LLM QA Fix Summary\"] = fix.get(\"Fix Summary\", \"Auto-fixed by Agent\")\n",
    "        return {\"row\": corrected}\n",
    "\n",
    "    except Exception:\n",
    "        return {\"row\": row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7982c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input_text: str\n",
    "    input_date: str\n",
    "    input_url: str\n",
    "\n",
    "    final_data: dict\n",
    "    final_rows: list\n",
    "    validated_rows: list\n",
    "    final_rows_post_llm: list\n",
    "\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "def stage_1_sourcing(state: AgentState):\n",
    "    res = sourcing_extractor.invoke({\n",
    "        \"paragraph\": state[\"input_text\"],\n",
    "        \"url\": state[\"input_url\"],\n",
    "        \"date\": state[\"input_date\"],\n",
    "    })\n",
    "    new_data = state.get(\"final_data\", {}).copy()\n",
    "    new_data.update(res)\n",
    "    return {\"final_data\": new_data}\n",
    "\n",
    "\n",
    "def stage_2_geography(state: AgentState):\n",
    "    res = geography_extractor.invoke({\"paragraph\": state[\"input_text\"]})\n",
    "    new_data = state.get(\"final_data\", {}).copy()\n",
    "    new_data.update(res)\n",
    "    return {\"final_data\": new_data}\n",
    "\n",
    "\n",
    "def stage_3_system(state: AgentState):\n",
    "    \"\"\"\n",
    "    NEW Stage3 Node:\n",
    "    Runs system classification PER split row using item_focus.\n",
    "    \"\"\"\n",
    "\n",
    "    paragraph = state[\"input_text\"]\n",
    "    rows = state.get(\"final_rows\", [])\n",
    "\n",
    "    updated_rows = []\n",
    "\n",
    "    for r in rows:\n",
    "        item_focus = str(r.get(\"System Name (Specific)\", \"\")).strip()\n",
    "\n",
    "        res = system_classifier.invoke({\n",
    "            \"paragraph\": paragraph,\n",
    "            \"item_focus\": item_focus\n",
    "        })\n",
    "\n",
    "        rr = r.copy()\n",
    "        rr.update(res)   # merge system labels into the row\n",
    "        updated_rows.append(rr)\n",
    "\n",
    "    return {\"final_rows\": updated_rows}\n",
    "\n",
    "\n",
    "def stage_4_contract(state: AgentState):\n",
    "    res = contract_extractor.invoke({\n",
    "        \"paragraph\": state[\"input_text\"],\n",
    "        \"contract_date\": state[\"input_date\"]\n",
    "    })\n",
    "    new_data = state.get(\"final_data\", {}).copy()\n",
    "    new_data.update(res)\n",
    "    return {\"final_data\": new_data}\n",
    "\n",
    "\n",
    "def stage_5_split(state: AgentState):\n",
    "    \"\"\"\n",
    "    Stage5 Node: SplitterEngine\n",
    "\n",
    "     Ensures output is ALWAYS stored in `final_rows`\n",
    "    so Stage3 SystemClassifierRAG can loop through them.\n",
    "    \"\"\"\n",
    "\n",
    "    base_row = state.get(\"final_data\", {}) or {}\n",
    "    paragraph = state.get(\"input_text\", \"\")\n",
    "\n",
    "    try:\n",
    "        res = splitter_agent.invoke({\n",
    "            \"paragraph\": paragraph,\n",
    "            \"base_row\": base_row\n",
    "        })\n",
    "\n",
    "        rows = res.get(\"rows\", None)\n",
    "\n",
    "        # Hard safety fallback\n",
    "        if not rows or not isinstance(rows, list):\n",
    "            rows = [base_row]\n",
    "\n",
    "        return {\"final_rows\": rows}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Never crash pipeline due to split failure\n",
    "        fallback = base_row.copy()\n",
    "        fallback[\"Split Flag\"] = \"Error\"\n",
    "        fallback[\"Split Reason\"] = f\"SplitterEngine failed: {str(e)}\"\n",
    "        return {\"final_rows\": [fallback]}\n",
    "\n",
    "\n",
    "def stage_6_quality_validator(state: AgentState):\n",
    "    res = quality_validator.invoke({\n",
    "        \"paragraph\": state[\"input_text\"],\n",
    "        \"rows\": state[\"final_rows\"]\n",
    "    })\n",
    "    return {\"validated_rows\": res.get(\"rows\", state[\"final_rows\"])}\n",
    "\n",
    "\n",
    "def stage_7_llm_fix_fail_rows(state: AgentState):\n",
    "    paragraph = state[\"input_text\"]\n",
    "    validated_rows = state.get(\"validated_rows\", [])\n",
    "\n",
    "    fixed_rows = []\n",
    "    for r in validated_rows:\n",
    "        if r.get(\"QA Status\") == \"FAIL\":\n",
    "            fix_res = llm_fail_row_validator.invoke({\"paragraph\": paragraph, \"row\": r})\n",
    "            fixed_rows.append(fix_res.get(\"row\", r))\n",
    "        else:\n",
    "            fixed_rows.append(r)\n",
    "\n",
    "    return {\"final_rows_post_llm\": fixed_rows}\n",
    "\n",
    "def node_system_classifier_rag(state: AgentState):\n",
    "    \"\"\"\n",
    "    Stage3 Node (AFTER SplitterEngine)\n",
    "\n",
    "    Runs system classification per split-row using:\n",
    "       item_focus = row[\"System Name (Specific)\"] or row[\"System Name (General)\"]\n",
    "\n",
    "    Updates each row with Market Segment / System Type / System Name / Evidence / Reason\n",
    "    \"\"\"\n",
    "\n",
    "    paragraph = state[\"input_text\"]\n",
    "    rows = state.get(\"final_rows\", [])\n",
    "\n",
    "    # If split engine didn't create rows, fallback to single final_data row\n",
    "    if not rows:\n",
    "        base = state.get(\"final_data\", {})\n",
    "        rows = [base] if base else []\n",
    "\n",
    "    updated_rows = []\n",
    "\n",
    "    for r in rows:\n",
    "        item_focus = str(r.get(\"System Name (Specific)\", \"\")).strip()\n",
    "        if not item_focus:\n",
    "            item_focus = str(r.get(\"System Name (General)\", \"\")).strip()\n",
    "\n",
    "        # invoke your Stage3 tool\n",
    "        sys_res = system_classifier.invoke({\n",
    "            \"paragraph\": paragraph,\n",
    "            \"item_focus\": item_focus\n",
    "        })\n",
    "\n",
    "        rr = r.copy()\n",
    "        rr.update(sys_res)\n",
    "        updated_rows.append(rr)\n",
    "\n",
    "    return {\"final_rows\": updated_rows}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d1af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"SourcingExtractor\", stage_1_sourcing)\n",
    "workflow.add_node(\"GeographyExtractor\", stage_2_geography)\n",
    "workflow.add_node(\"ContractExtractor\", stage_4_contract)\n",
    "workflow.add_node(\"SplitterEngine\", stage_5_split)\n",
    "workflow.add_node(\"SystemClassifierRAG\", node_system_classifier_rag)  # \n",
    "workflow.add_node(\"QualityValidator\", stage_6_quality_validator)\n",
    "workflow.add_node(\"LLMFailRowFixer\", stage_7_llm_fix_fail_rows)\n",
    "\n",
    "workflow.add_edge(START, \"SourcingExtractor\")\n",
    "workflow.add_edge(\"SourcingExtractor\", \"GeographyExtractor\")\n",
    "workflow.add_edge(\"GeographyExtractor\", \"ContractExtractor\")\n",
    "workflow.add_edge(\"ContractExtractor\", \"SplitterEngine\")   # \n",
    "workflow.add_edge(\"SplitterEngine\", \"SystemClassifierRAG\") # \n",
    "workflow.add_edge(\"SystemClassifierRAG\", \"QualityValidator\")\n",
    "workflow.add_edge(\"QualityValidator\", \"LLMFailRowFixer\")\n",
    "workflow.add_edge(\"LLMFailRowFixer\", END)\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac0b5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reach https://mermaid.ink API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "  display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4afc2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) GRAPH EXPORT (OFFLINE SAFE)\n",
    "\n",
    "def export_workflow_mermaid(app_obj, out_file=\"workflow.mmd\"):\n",
    "    mmd = app_obj.get_graph().draw_mermaid()\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(mmd)\n",
    "    print(f\"Workflow Mermaid saved locally: {out_file}\")\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be954ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading Input File: C:\\Users\\mukeshkr\\Agentic-AI-Defense-Data-Extraction\\data\\source_file.xlsx\n",
      "Workflow Mermaid saved locally: workflow.mmd\n",
      "Processing 30 rows...\n",
      "\n",
      " Row 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukeshkr\\AppData\\Local\\Temp\\ipykernel_12420\\604486570.py:212: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  start = pd.to_datetime(start_date_str, dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Row 2/30\n",
      "\n",
      " Row 3/30\n",
      "\n",
      " Row 4/30\n",
      "\n",
      " Row 5/30\n",
      "\n",
      " Row 6/30\n",
      "\n",
      " Row 7/30\n",
      "\n",
      " Row 8/30\n",
      "\n",
      " Row 9/30\n",
      "\n",
      " Row 10/30\n",
      "\n",
      " Row 11/30\n",
      "\n",
      " Row 12/30\n",
      "\n",
      " Row 13/30\n",
      "\n",
      " Row 14/30\n",
      "\n",
      " Row 15/30\n",
      "\n",
      " Row 16/30\n",
      "\n",
      " Row 17/30\n",
      "\n",
      " Row 18/30\n",
      "\n",
      " Row 19/30\n",
      "\n",
      " Row 20/30\n",
      "\n",
      " Row 21/30\n",
      "\n",
      " Row 22/30\n",
      "\n",
      " Row 23/30\n",
      "\n",
      " Row 24/30\n",
      "\n",
      " Row 25/30\n",
      "\n",
      " Row 26/30\n",
      "\n",
      " Row 27/30\n",
      "\n",
      " Row 28/30\n",
      "\n",
      " Row 29/30\n",
      "\n",
      " Row 30/30\n",
      "Saving to Excel: Processed_Defense_Data.xlsx\n",
      "Applying Highlighting...\n",
      "Evidence + Reason columns highlighted successfully.\n",
      "\n",
      "Processing Complete!\n",
      "                    Supplier Name Value (USD$ Million) Customer Operator Supplier Region\n",
      "BAE Systems - Norfolk Ship Repair              107.736              Navy   North America\n",
      "                           Boeing               25.648              Navy   North America\n",
      "                         Intermat               17.736              Navy   North America\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f\"\\n Loading Input File: {INPUT_EXCEL_PATH}\")\n",
    "    \n",
    "    # Define Output Path as Excel\n",
    "    OUTPUT_EXCEL_PATH = \"Processed_Defense_Data.xlsx\"\n",
    "\n",
    "    export_workflow_mermaid(app, out_file=\"workflow.mmd\")\n",
    "\n",
    "    try:\n",
    "        df_input = pd.read_excel(INPUT_EXCEL_PATH)\n",
    "        \n",
    "        # Basic validation\n",
    "        required_cols = [\"Source URL\", \"Contract Date\", \"Contract Description\"]\n",
    "        # Allow loose matching or strip whitespace from columns if needed\n",
    "        df_input.columns = [c.strip() for c in df_input.columns]\n",
    "        \n",
    "        if not all(col in df_input.columns for col in required_cols):\n",
    "             raise ValueError(f\"Excel file must contain columns: {required_cols}\")\n",
    "\n",
    "        print(f\"Processing {len(df_input)} rows...\")\n",
    "        results = []\n",
    "\n",
    "        for index, row in df_input.iterrows():\n",
    "            print(f\"\\n Row {index + 1}/{len(df_input)}\")\n",
    "\n",
    "            desc = str(row[\"Contract Description\"]) if pd.notna(row[\"Contract Description\"]) else \"\"\n",
    "            c_date = str(row[\"Contract Date\"]) if pd.notna(row[\"Contract Date\"]) else str(datetime.date.today())\n",
    "            c_url = str(row[\"Source URL\"]) if pd.notna(row[\"Source URL\"]) else \"\"\n",
    "\n",
    "            initial_state: AgentState = {\n",
    "                \"input_text\": desc,\n",
    "                \"input_date\": c_date,\n",
    "                \"input_url\": c_url,\n",
    "                \"final_data\": {},\n",
    "                \"final_rows\": [],\n",
    "                \"validated_rows\": [],\n",
    "                \"final_rows_post_llm\": [],\n",
    "                \"messages\": []\n",
    "            }\n",
    "\n",
    "            output_state = app.invoke(initial_state)\n",
    "\n",
    "            # Hierarchy of fallback for getting rows\n",
    "            rows = output_state.get(\"final_rows_post_llm\", [])\n",
    "            if not rows:\n",
    "                rows = output_state.get(\"validated_rows\", [])\n",
    "            if not rows:\n",
    "                rows = output_state.get(\"final_rows\", [])\n",
    "            if not rows:\n",
    "                rows = [output_state.get(\"final_data\", {})]\n",
    "\n",
    "            results.extend(rows)\n",
    "\n",
    "        df_final = pd.DataFrame(results)\n",
    "\n",
    "        FINAL_COLUMNS = [\n",
    "            \"Customer Region\", \"Customer Country\", \"Customer Operator\",\n",
    "            \"Supplier Region\", \"Supplier Country\", \"Domestic Content\",\n",
    "\n",
    "            \"Split Flag\", \"Split Reason\",\n",
    "\n",
    "            \"Market Segment\", \"Market Segment Evidence\", \"Market Segment Reason\",\n",
    "            \"System Type (General)\", \"System Type (General) Evidence\", \"System Type (General) Reason\",\n",
    "            \"System Type (Specific)\", \"System Type (Specific) Evidence\", \"System Type (Specific) Reason\",\n",
    "            \"System Name (General)\", \"System Name (General) Evidence\", \"System Name (General) Reason\",\n",
    "            \"System Name (Specific)\", \"System Name (Specific) Evidence\", \"System Name (Specific) Reason\",\n",
    "            \"System Piloting\", \"System Piloting Evidence\", \"System Piloting Reason\",\n",
    "            \"Confidence\",\n",
    "\n",
    "            \"Supplier Name\", \"Supplier Name Evidence\",\n",
    "            \"Program Type\", \"Expected MRO Contract Duration (Months)\",\n",
    "            \"Quantity\", \"Value Certainty\", \"Value (Million)\", \"Currency\",\n",
    "            \"Value (USD$ Million)\", \"Value Note (If Any)\", \"G2G/B2G\",\n",
    "            \"Signing Month\", \"Signing Year\",\n",
    "\n",
    "            \"QA Status\", \"QA Flags\", \"QA Fix Suggestion\",\n",
    "            \"LLM QA Fix Summary\",\n",
    "\n",
    "            \"Description of Contract\",\n",
    "            \"Additional Notes (Internal Only)\",\n",
    "            \"Source Link(s)\",\n",
    "            \"Contract Date\",\n",
    "            \"Reported Date (By SGA)\"\n",
    "        ]\n",
    "\n",
    "        # Reindex ensures all columns exist, filling missing ones with empty string\n",
    "        df_final = df_final.reindex(columns=FINAL_COLUMNS, fill_value=\"\")\n",
    "\n",
    "        # 1. Save as Excel\n",
    "        print(f\"Saving to Excel: {OUTPUT_EXCEL_PATH}\")\n",
    "        df_final.to_excel(OUTPUT_EXCEL_PATH, index=False, engine='openpyxl')\n",
    "        \n",
    "        # 2. Apply Formatting/Highlighting\n",
    "        print(\"Applying Highlighting...\")\n",
    "        highlight_evidence_reason_columns(OUTPUT_EXCEL_PATH)\n",
    "\n",
    "        print(\"\\nProcessing Complete!\")\n",
    "        print(df_final[[\"Supplier Name\", \"Value (USD$ Million)\", \"Customer Operator\", \"Supplier Region\"]].head(3).to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
